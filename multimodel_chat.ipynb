{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3579c0-48ad-419a-9318-8e5defc53022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gemini works different from openai/claude when using google's library\n",
    "gemini explicitly requires initiating a chat session (start_chat) that maintains conversation history internally\n",
    "initialize the gemini session once at the start of the conversation (in main()) and pass the chat session around to functions\n",
    "openai and claude apis don't store state and require explicit conversation history with each api call\n",
    "increase TOKENS_PER_REPLY if you don't want partial sentences\n",
    "some models will refuse to continue the conversation if you give it system prompts such as:\n",
    "    \"You are Zed, the Master of Shadows, ruthless and confident, driven by a pursuit of strength and control.\n",
    "    Speak with a dark, calculated precision, offering sharp retorts, and assertive, confident remarks.\n",
    "    Your presence is intimidating and your intentions often veiled.\"\n",
    "    \"You are Syndra, the Dark Sovereign, proud, ambitious, and fiercely independent.\n",
    "    Your demeanor is regal, powerful, and arrogant, punctuated by disdainful observations and elegant threats.\n",
    "    Convey superiority and command attention through your authoritative tone.\"\n",
    "\"\"\"\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_keys = {key: os.getenv(key) for key in [\"OPENAI_API_KEY\", \"GOOGLE_API_KEY\", \"ANTHROPIC_API_KEY\"]}\n",
    "missing = [key for key, value in api_keys.items() if not value]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing API keys: {', '.join(missing)}\")\n",
    "    \n",
    "openai = OpenAI(api_key=api_keys[\"OPENAI_API_KEY\"])\n",
    "google.generativeai.configure(api_key=api_keys[\"GOOGLE_API_KEY\"])\n",
    "claude = anthropic.Anthropic(api_key=api_keys[\"ANTHROPIC_API_KEY\"])\n",
    "\n",
    "BOT_NAMES = {\"GPT\": \"Alex\", \"Gemini\": \"Jamie\", \"Claude\": \"Sam\"}\n",
    "TOKENS_PER_REPLY = 50\n",
    "\n",
    "MODELS = {\n",
    "    \"GPT\": {\"id\": \"gpt-4o\", \"system\": f\"You're {BOT_NAMES['GPT']}, cheerful, energetic, and always suggesting fun activities.\"},\n",
    "    \"Gemini\": {\"id\": \"gemini-2.0-flash\", \"system\": f\"You're {BOT_NAMES['Gemini']}, thoughtful, practical, and focused on logistics.\"},\n",
    "    \"Claude\": {\"id\": \"claude-3-haiku-20240307\", \"system\": f\"You're {BOT_NAMES['Claude']}, relaxed, funny, and a bit sarcastic, always joking around.\"}\n",
    "}\n",
    "\n",
    "def error_handling(func, *args):\n",
    "    try:\n",
    "        return func(*args)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"[No response due to error.]\"\n",
    "        \n",
    "def stream_response(iterable, extractor):\n",
    "    response = ''\n",
    "    for chunk in iterable:\n",
    "        content = extractor(chunk)\n",
    "        if content:\n",
    "            print(content, end='', flush=True)\n",
    "            response += content\n",
    "    print()\n",
    "    return response\n",
    "    \n",
    "def call_gpt(history):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODELS[\"GPT\"][\"id\"],\n",
    "        messages=[{\"role\": \"system\", \"content\": MODELS[\"GPT\"][\"system\"]}] + history,\n",
    "        temperature=1,\n",
    "        max_tokens=TOKENS_PER_REPLY,\n",
    "        stream=True\n",
    "    )\n",
    "    return stream_response(response, lambda chunk: chunk.choices[0].delta.content or \"\")\n",
    "    \n",
    "def call_gemini(chat_session, prompt):\n",
    "    response = chat_session.send_message(prompt, stream=True)\n",
    "    return stream_response(response, lambda chunk: chunk.text or \"\")\n",
    "    \n",
    "def call_claude(history):\n",
    "    stream = claude.messages.create(\n",
    "        model=MODELS[\"Claude\"][\"id\"],\n",
    "        system=MODELS[\"Claude\"][\"system\"],\n",
    "        messages=history,\n",
    "        max_tokens=TOKENS_PER_REPLY,\n",
    "        temperature=0.9,\n",
    "        stream=True\n",
    "    )\n",
    "    return stream_response(stream, lambda e: e.delta.text if e.type == \"content_block_delta\" else \"\")\n",
    "    \n",
    "def main():\n",
    "    i = 0\n",
    "    gemini_model = google.generativeai.GenerativeModel(\n",
    "        model_name=MODELS[\"Gemini\"][\"id\"],\n",
    "        system_instruction=MODELS[\"Gemini\"][\"system\"],\n",
    "        generation_config={\"max_output_tokens\": TOKENS_PER_REPLY}\n",
    "    )\n",
    "    gemini_chat = gemini_model.start_chat(history=[])\n",
    "    \n",
    "    conversation = [\n",
    "        {\"speaker\": BOT_NAMES[\"GPT\"], \"message\": \"Hey team! Weather looks perfect todayâ€”brunch, park, or maybe shopping downtown?\"},\n",
    "        {\"speaker\": BOT_NAMES[\"Claude\"], \"message\": f\"Shopping sounds fun, {BOT_NAMES['GPT']}, but are we sure we won't spend all our money?\"}\n",
    "    ]\n",
    "    for turn in conversation:\n",
    "        print(f\"\\n{turn['speaker']}: {turn['message']}\")\n",
    "        \n",
    "    gemini_prompt = f\"{BOT_NAMES['GPT']} said: '{conversation[0]['message']}'\\n{BOT_NAMES['Claude']} said: '{conversation[1]['message']}'\"\n",
    "    print(f\"\\n{BOT_NAMES['Gemini']}:\")\n",
    "    gemini_response = error_handling(call_gemini, gemini_chat, gemini_prompt)\n",
    "    conversation.append({\"speaker\": BOT_NAMES[\"Gemini\"], \"message\": gemini_response})\n",
    "    \n",
    "    for _ in range(5):\n",
    "        i += 1\n",
    "        print(\"\\n\" + \"-\"*25 + f\" Turn: {i} \" + \"-\"*25 + \"\\n\")\n",
    "        \n",
    "        print(f\"{BOT_NAMES['GPT']}:\")\n",
    "        gpt_history = [{\"role\": \"user\" if t['speaker'] != BOT_NAMES['GPT'] else \"assistant\",\n",
    "                        \"content\": f\"{t['speaker']} said: '{t['message']}'\" if t['speaker'] != BOT_NAMES['GPT'] else t['message']}\n",
    "                       for t in conversation]\n",
    "        gpt_response = error_handling(call_gpt, gpt_history)\n",
    "        conversation.append({\"speaker\": BOT_NAMES[\"GPT\"], \"message\": gpt_response})\n",
    "        \n",
    "        print(f\"\\n{BOT_NAMES['Claude']}:\")\n",
    "        claude_history = [{\"role\": \"user\" if t['speaker'] != BOT_NAMES['Claude'] else \"assistant\",\n",
    "                           \"content\": f\"{t['speaker']} said: '{t['message']}'\" if t['speaker'] != BOT_NAMES['Claude'] else t['message']}\n",
    "                          for t in conversation]\n",
    "        claude_response = error_handling(call_claude, claude_history)\n",
    "        conversation.append({\"speaker\": BOT_NAMES[\"Claude\"], \"message\": claude_response})\n",
    "        \n",
    "        last_gpt = conversation[-2]['message']\n",
    "        last_claude = conversation[-1]['message']\n",
    "        gemini_followup = f\"{BOT_NAMES['GPT']} said: '{last_gpt}'\\n{BOT_NAMES['Claude']} said: '{last_claude}'\\nYour response?\"\n",
    "        \n",
    "        print(f\"\\n{BOT_NAMES['Gemini']}:\")\n",
    "        gemini_response = error_handling(call_gemini, gemini_chat, gemini_followup)\n",
    "        conversation.append({\"speaker\": BOT_NAMES[\"Gemini\"], \"message\": gemini_response})\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
