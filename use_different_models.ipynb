{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- From ChatGPT -----\n",
      "I told my computer I needed a break, and now it won't stop sending me beach wallpaper!\n",
      "\n",
      "----- From Gemini -----\n",
      "Why don't scientists trust atoms? Because they make up everything!\n",
      "\n",
      "...I'll be here all week, try the veal! Though, technically, I'm always here... and I don't eat veal. Existential, isn't it?\n",
      "\n",
      "\n",
      "----- From Gemini through OpenAI -----\n",
      "Why don't scientists trust atoms? Because they make up everything! \n",
      "\n",
      "... I'll be here all week, try the veal! (Though, I'm not sure what veal even *is*. Sounds suspicious).\n",
      "\n",
      "\n",
      "----- From Claude -----\n",
      "Hmm, that's a tough one! I have so many good ones, it's hard to choose. Maybe something like \"I'm not saying I'm Batman, but have you ever seen me and Batman in the same room together?\" Or how about \"I'm not superstitious, but I am a little stitious.\" ðŸ˜‰ I do love a good pun or a clever quip. What's your favorite one-liner?\n",
      "\n",
      "----- From DeepSeek -----\n",
      "Oh, thatâ€™s easy: *\"I told my wife she was drawing her eyebrows too highâ€¦ she looked surprised.\"*  \n",
      "\n",
      "Itâ€™s a classicâ€”short, sharp, and leaves just enough room for the groan to sink in! ðŸ˜† Got a topic youâ€™d like me to roast with a one-liner?"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "missing_keys = [\n",
    "    key for key in [\"OPENAI_API_KEY\", \"GOOGLE_API_KEY\", \"ANTHROPIC_API_KEY\", \"DEEPSEEK_API_KEY\"]\n",
    "    if not os.getenv(key)\n",
    "]\n",
    "if missing_keys:\n",
    "    raise ValueError(f\"Missing keys in .env: {', '.join(missing_keys)}\")\n",
    "\n",
    "openai = OpenAI(api_key=openai_api_key)\n",
    "google.generativeai.configure(api_key=google_api_key)\n",
    "claude = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "system_message = \"You are a witty and creative assistant known for hilarious one-liners.\"\n",
    "user_prompt = \"What's your absolute favorite one-liner?\"\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "print(\"----- From ChatGPT -----\")\n",
    "try:\n",
    "    openai_response = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=prompts,\n",
    "        temperature=0.7,\n",
    "        stream=True\n",
    "    )\n",
    "    for chunk in openai_response:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            print(content, end='', flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"\\nError from OpenAI: {e}\")\n",
    "\n",
    "# gemini api is different\n",
    "print(\"\\n\\n----- From Gemini -----\")\n",
    "try:\n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "        model_name='gemini-2.0-flash',\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "    gemini_response = gemini.generate_content(user_prompt, stream=True)\n",
    "    for chunk in gemini_response:\n",
    "        print(chunk.text, end='', flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"\\nError from Gemini: {e}\")\n",
    "\n",
    "# you can use gemini with the openai library\n",
    "print(\"\\n\\n----- From Gemini through OpenAI -----\")\n",
    "try:\n",
    "    gemini_openai_response = gemini_via_openai_client.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        messages=prompts,\n",
    "        temperature=0.7,\n",
    "        stream=True\n",
    "    )\n",
    "    for chunk in gemini_openai_response:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            print(content, end='', flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"\\nError from Gemini via OpenAI: {e}\")\n",
    "\n",
    "print(\"\\n\\n----- From Claude -----\")\n",
    "try:\n",
    "    claude_response = claude.messages.stream(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=200,\n",
    "        temperature=0.7,\n",
    "        system=system_message,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "    )\n",
    "    with claude_response as stream:\n",
    "        for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"\\nError from Claude: {e}\")\n",
    "\n",
    "print(\"\\n\\n----- From DeepSeek -----\")\n",
    "try:\n",
    "    deepseek_response = deepseek_via_openai_client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=prompts,\n",
    "        stream=True\n",
    "    )\n",
    "    for chunk in deepseek_response:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            print(content, end='', flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"\\nError from DeepSeek: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf72f2-f626-44d2-8fd1-9f803da7b544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
